{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Online Learning with SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define run parameter (can be accessed by papermill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FULL_RUN = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Load data\n",
    "\n",
    "Kaggle [Avazu CTR Prediction challenge](https://www.kaggle.com/c/avazu-ctr-prediction)\n",
    "\n",
    "Look at size of training set and number of lines. For this, use the following commands:\n",
    "\n",
    "```bash\n",
    "du -h train.csv\n",
    "wc -l train.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We'll go through this data by \"chunks\". Let's set the size of a chunk (can be accessed by papermill):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_SIZE = 1000 # small value for fast execution; can bump to 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a chunk \"reader\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.utils.data import filename2path\n",
    "from pandas import read_csv\n",
    "TRAIN_FILE = filename2path(\"avazu\", \"train_full_raw.csv\")\n",
    "HEADER = ['id','click','hour','C1','banner_pos','site_id','site_domain','site_category','app_id','app_domain','app_category','device_id'\\\n",
    "        ,'device_ip','device_model','device_type','device_conn_type','C14','C15','C16','C17','C18','C19','C20','C21']\n",
    "reader = read_csv(TRAIN_FILE, chunksize=CHUNK_SIZE, names=HEADER, header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "There are about 4,000 chunks of size 10,000 in this data.\n",
    "\n",
    "Let's look at the first chunk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk = reader.get_chunk()\n",
    "print(\"Chunk weighs \" + str(chunk.memory_usage(index=True).sum()/(1024**2)) + \" MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commenting out this block - waiting for qgrid to be compatible with the latest version of pandas\n",
    "# import qgrid\n",
    "# qgrid_widget = qgrid.show_grid(chunk, show_toolbar=True)\n",
    "# qgrid_widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Prepare data\n",
    "\n",
    "Let's create a function that\n",
    "\n",
    "* Extracts inputs `X` and outputs `y` from a chunk dataframe\n",
    "* Applies feature hashing to the inputs (here, using $2^{20}$ features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import asarray\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "fh = FeatureHasher(n_features=2**20, input_type='string')\n",
    "\n",
    "def chunk2Xy(chunk):\n",
    "    y = chunk['click'].values\n",
    "    X = chunk.drop(['id', 'click'], axis=1) # remove id and target columns\n",
    "    X = fh.transform(asarray(X.astype(str))) # transform X to array of strings, so we can apply feature hashing\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Test this function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk2Xy(reader.get_chunk())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Set model hyper-parameters\n",
    "\n",
    "We'll use the first INIT_SIZE lines of the dataset as training data to tune hyper-parameters:\n",
    "\n",
    "* SGD parameters: learning rate and number of epochs\n",
    "* model parameters: regularization\n",
    "* featurization parameters: number of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Re-set `reader` so we start reading from the beginning of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = read_csv(TRAIN_FILE, chunksize=CHUNK_SIZE, names=HEADER, header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose INIT_SIZE so that training data fits in memory and search isn't too long (can be accessed by papermill):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INIT_SIZE = 100000 # small value for fast execution; can bump to 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_init = reader.get_chunk(INIT_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Chunk weighs \" + str(chunk_init.memory_usage(index=True).sum()/(1024**2)) + \" MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Hyper-parameter tuning..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at output distribution\n",
    "# split X, y into train and val sets\n",
    "# fit on train\n",
    "# compute accuracy on val set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1-sum(chunk_init['click'].values)/len(chunk_init['click'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X, y = chunk2Xy(chunk_init)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "model = SGDClassifier(loss='log', random_state=42, max_iter=100)\n",
    "classes = [0, 1]\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "y_pred_val = model.predict(X_val)\n",
    "y_pred_proba_val = model.predict_proba(X_val)\n",
    "print(\"Accuracy: \" + str(metrics.accuracy_score(y_val, y_pred_val)))\n",
    "print(\"F-measure: \" + str(metrics.f1_score(y_val, y_pred_val)))\n",
    "print(\"log-loss: \" + str(metrics.log_loss(y_val, y_pred_proba_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fit model chunk-by-chunk\n",
    "\n",
    "Let's iterate over the chunk reader, update our model with every new chunk, and display logloss every K chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's set K to 100 and initialize a variable to store loss values every K chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 100\n",
    "chunk_indices_processed = []\n",
    "losses_Kchunks = []\n",
    "# print(\"The number of loss values will grow in size from 0 to \" + N/(CHUNK_SIZE*K))\n",
    "print(\"Each loss value will be computed over \" + str(CHUNK_SIZE*K) + \" data points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize variables to store outputs and predictions for these data points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_Kchunks = []\n",
    "y_pred_Kchunks = []\n",
    "y_pred_no_update_Kchunks = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Define a function that plots the loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from pandas import DataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def display_loss():\n",
    "    clear_output()\n",
    "    DataFrame({'linear_regression': losses_Kchunks}).plot()\n",
    "    plt.xlabel('Number of times (K chunks) were processed (K=' + str(K) + ')')\n",
    "    plt.ylabel('Log-loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Iterate on `reader`, apply model to new chunk so we can update `y_pred_Kchunks`, and then update it using the new chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "for c, chunk in enumerate(reader): # c is a chunks counter\n",
    "\n",
    "    X, y = chunk2Xy(chunk)\n",
    "    \n",
    "    # Compute loss over the last K chunks\n",
    "    # Step 1. Update lists of outputs and predictions at every chunk\n",
    "    y_Kchunks.extend(y)\n",
    "    y_pred_Kchunks.extend(model.predict_proba(X))\n",
    "    # Step 2. Every K chunks...\n",
    "    if (c % K == 0):\n",
    "        # Display loss over the last K chunks\n",
    "        chunk_indices_processed.append(c)\n",
    "        losses_Kchunks.append(log_loss(y_Kchunks, y_pred_Kchunks))\n",
    "        # losses_Kchunks_no_update.append(log_loss(y_Kchunks, y_pred_no_update_Kchunks))\n",
    "        display_loss()\n",
    "        \n",
    "        # Reset lists of outputs and predictions used to compute loss\n",
    "        y_Kchunks = []\n",
    "        y_pred_Kchunks = []\n",
    "        # y_pred_no_update_Kchunks = []\n",
    "\n",
    "    model.partial_fit(X, y, classes=classes) # this runs only 1 epoch; try more?\n",
    "\n",
    "    # if not doing a full run, stop loop after 2*K chunks\n",
    "    if (not FULL_RUN and c==2*K):\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,scripts//py"
  },
  "kernelspec": {
   "display_name": "full-stack-ml",
   "language": "python",
   "name": "full-stack-ml"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
