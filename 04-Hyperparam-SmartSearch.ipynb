{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hyper-parameter tuning with Smart Search\n",
    "\n",
    "All hyper-parameters that influence the learning are searched simultaneously (except for the number of estimators, which poses a time / quality tradeoff)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Notebook variables\n",
    "\n",
    "(Can be accessed by papermill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "EVALS = 4 # the number of evaluations roughly determines how long search procedures will take; this value is just for tests, it should be increased for meaningful results!\n",
    "DATASET = \"digits\" # \"digits\" (rather small, good to start running experiments) or \"credit\" (bigger)\n",
    "FOLDS = 3 # smaller values will make the evaluation quicker; recommended values are between 3 and 10\n",
    "NTREES = 10 # reasonable values between 10 and 100; this has an impact on the time an evaluation takes\n",
    "ESTIMATOR = \"rf\" # \"rf\" or \"xgb\"\n",
    "HP_DIST = { # this is for rf\n",
    "    \"max_depth\": {\n",
    "        \"type\": \"int\",\n",
    "        \"min\": 2,\n",
    "        \"max\": 11\n",
    "    },\n",
    "    \"max_features\":{\n",
    "        \"type\": \"float\",\n",
    "        \"min\": 0.1,\n",
    "        \"max\": 1.0\n",
    "    },\n",
    "    \"bootstrap\": {\n",
    "        \"type\": \"choice\",\n",
    "        \"values\": [True, False]\n",
    "    }\n",
    "}\n",
    "SCORING = \"neg_log_loss\" # negative log likelihood\n",
    "SEED = 42 # for reproducibility of results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Default model\n",
    "\n",
    "Initialize an empty model with specified number of trees (`NTREES`) and random state (`SEED`), and with default hyper-parameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "if (ESTIMATOR==\"rf\"):\n",
    "    default_params = {\"n_estimators\": NTREES, \"random_state\": SEED}\n",
    "    default_model = RandomForestClassifier(**default_params)\n",
    "elif (ESTIMATOR==\"xgb\"):\n",
    "    default_params = {\"n_estimators\": NTREES, \"seed\": SEED}\n",
    "    default_model = XGBClassifier(**default_params)\n",
    "    if (DATASET==\"credit\"):\n",
    "        default_params.update({\"objective\": \"multi:softprob\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data\n",
    "\n",
    "Import data and prepare X and y (inputs and outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (DATASET==\"digits\"):\n",
    "    from sklearn.datasets import load_digits\n",
    "    digits = load_digits()\n",
    "    X, y = digits.data, digits.target\n",
    "elif (DATASET==\"credit\"):\n",
    "    from mlxtend.data.kaggle_gmsc import kaggle_gmsc_data_nomissing\n",
    "    X, y = kaggle_gmsc_data_nomissing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hyper-parameter distributions\n",
    "\n",
    "Create hyperopt-compatible distributions based on values stored in `HP_DIST`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp     \n",
    "hp_space = {}\n",
    "for v in HP_DIST.keys():\n",
    "    if HP_DIST[v]['type']==\"int\":\n",
    "        hp_space[v] = hp.quniform(v, HP_DIST[v]['min'], HP_DIST[v]['max'], 1)\n",
    "    elif HP_DIST[v]['type']==\"float\":\n",
    "        hp_space[v] = hp.uniform(v, HP_DIST[v]['min'], HP_DIST[v]['max'])\n",
    "    elif HP_DIST[v]['type']==\"choice\":\n",
    "        hp_space[v] = hp.choice(v, HP_DIST[v][\"values\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperopt uses float, but when parameters are actually int, scikit needs them to be int!\n",
    "\n",
    "Let's create a function that \"converts\" hyperopt-compatible parameters to scikit-compatible parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperopt2scikit_params(h_params):\n",
    "    s_params = h_params\n",
    "    for v in HP_DIST.keys():\n",
    "        if HP_DIST[v]['type']==\"int\":\n",
    "            s_params[v] = int(h_params[v])\n",
    "    s_params.update(default_params)\n",
    "    return s_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Define loss to minimize\n",
    "\n",
    "Hyperopt aims at minimizing the loss incurred by using a certain set of hyperparameter values. We define the loss function to use, which implements the evaluation of these hyperparameter values and requires access to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import STATUS_OK\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def hp_loss(hp_params):\n",
    "    s_params = hyperopt2scikit_params(hp_params)\n",
    "    if (ESTIMATOR==\"xgb\"):\n",
    "        model = XGBClassifier(**s_params)\n",
    "    elif (ESTIMATOR==\"rf\"):\n",
    "        model = RandomForestClassifier(**s_params)\n",
    "    s = cross_val_score(model, X, y, scoring=SCORING, cv=FOLDS, n_jobs=-1, verbose=1)\n",
    "    return {'loss': -s.mean(), 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Run minimization\n",
    "\n",
    "With hyperopt, we pass the hyper-parameter distributions to use, along with the loss function to minimize.\n",
    "\n",
    "Note the differences in design compared to scikit's RandomizedSearch (where we just pass the data via a `fit` function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, Trials\n",
    "from numpy.random import RandomState\n",
    "\n",
    "trials = Trials() # this is where the history will be saved\n",
    "%time best = fmin(hp_loss, hp_space, algo=tpe.suggest, trials=trials, max_evals=EVALS) # TODO: specify seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperopt2scikit_params(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Note on using `neg_log_loss` as our performance metric: we expect to see loss values which are > 0 since we're dealing with negative log likelihood (likelihood is between 0 and 1 and log makes those values negative); the smaller the loss, the higher the likelihood, the better!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from joblib import dump\n",
    "dump(trials, \"output/smart_search_\" + DATASET + \".joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Review results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "trials.losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials.best_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials.best_trial['misc']['vals']"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,scripts//py"
  },
  "kernelspec": {
   "display_name": "full-stack-ml",
   "language": "python",
   "name": "full-stack-ml"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
